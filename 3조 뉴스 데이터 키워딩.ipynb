{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3조 News 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbs(date):\n",
    "    \n",
    "    base_url = 'https://news.sbs.co.kr/news/newsSection.do'\n",
    "\n",
    "    params = {\n",
    "        'sectionType' : '01',\n",
    "        'plink' : 'SNB',\n",
    "        'cooper' : 'SBSNEWS',\n",
    "        'pageIdx' : 1,\n",
    "        'pageDate' : date\n",
    "    }\n",
    "\n",
    "    index_list = ['01', '02', '03', '07', '08', '14', '09'] # SectionType ( 카테고리를 가져오기 위한 리스트)\n",
    "\n",
    "    index_str_list = ['정치', '경제', '사회', '국제', '생활&문화', '연예', '스포츠'] # SectionType = 카테고리 이름\n",
    "\n",
    "    txt_file_name = ['a1.txt', 'b1.txt', 'c1.txt', 'd1.txt', 'e1.txt', 'f1.txt', 'g1.txt'] #각 카테고리별 저장을 위한 text 파일 list\n",
    "\n",
    "    \n",
    "    index_point = 0 # 리스트를 가져 오기 위한 index\n",
    "    \n",
    "    f = open(txt_file_name[index_point], \"w+t\") # 무조건 처음 시작할 때 지우고 다시만든다.\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        resp = requests.get(base_url, params = params)\n",
    "        soup = BeautifulSoup(resp.text, 'lxml')\n",
    "        news_list_tags = soup.select('#container > div > div.w_news_list.type_issue > ul > li > a.news')\n",
    "\n",
    "        if(len(news_list_tags) == 0): # 더이상 탐색할 페이지가 없을때\n",
    "\n",
    "            if(index_point + 1 == 7): # section 및 page 다 탐색 -> 종료\n",
    "                f.close()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                f.close()\n",
    "                index_point += 1\n",
    "                f = open(txt_file_name[index_point], \"w+t\")\n",
    "                \n",
    "                params['sectionType'] = index_list[index_point]\n",
    "                params['pageIdx'] = 1\n",
    "\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        for i in news_list_tags: # 파일을 쓴다.\n",
    "\n",
    "            dt_tags = i.find_all('strong')\n",
    "            title_tag = dt_tags[0].text\n",
    "            f.write(title_tag)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        params['pageIdx'] += 1\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jtbc_news_list(day):\n",
    "    \n",
    "    # 카테고리 'scode'를 dict로 정의.\n",
    "    menu_list = {\n",
    "        '속보' : 0,\n",
    "        '정치' : 10,\n",
    "        '경제' : 20,\n",
    "        '사회' : 30,\n",
    "        '국제' : 40,\n",
    "        '문화' : 50,\n",
    "        '연예' : 60,\n",
    "        '스포츠' : 70,\n",
    "        '날씨': 80\n",
    "    }\n",
    "    \n",
    "    base_url ='http://news.jtbc.joins.com/section/list.aspx?'\n",
    "    params = {\n",
    "        'scode' : 0,\n",
    "        'pdate' : day,\n",
    "        'pgi' : 0\n",
    "    }\n",
    "    \n",
    "    # text 파일 통합 위해서 이름명 설정. 날씨 카테고리 문화에 합침.\n",
    "    scode_list = {\n",
    "        10 : 'a',\n",
    "        20 : 'b',\n",
    "        30 : 'c',\n",
    "        40 : 'd',\n",
    "        50 : 'e',\n",
    "        60 : 'f',\n",
    "        70 : 'g',\n",
    "        80 : 'e'\n",
    "    }\n",
    "    \n",
    "    for k in range(8):\n",
    "        news_list = []\n",
    "        params['scode'] +=10 # 카테고리 넘기기\n",
    "        params['pgi'] = 0\n",
    "        \n",
    "        while True:\n",
    "            params['pgi'] +=1 # 페이지 넘기기\n",
    "\n",
    "            resp = requests.get(base_url, params=params)\n",
    "            soup = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "            news_tags = soup.find('ul', id='section_list')\n",
    "            news_list_tags = news_tags.find_all('dt', class_='title_cr') # 뉴스 타이틀 tag\n",
    "\n",
    "\n",
    "            if len(news_list_tags)==0: # 마지막 페이지에서 더 넘어가면 while문 탈출.\n",
    "                break\n",
    "\n",
    "            for tags in news_list_tags:\n",
    "                news_title = tags.text\n",
    "\n",
    "                news_list.append(\n",
    "                    news_title.strip()\n",
    "                )\n",
    "        \n",
    "    \n",
    "        file_name = scode_list[params['scode']]+'2.txt' # 카테고리별 params 숫자를 받아 파일명 key값으로 입력 -> 알파벳 출력\n",
    "        \n",
    "        \n",
    "        # 날씨 카테고리 내용 문화영역에 추가하기. 나머지는 그냥 파일 생성.\n",
    "        if params['scode'] == 80:\n",
    "            file1 = open(file_name, 'at')\n",
    "        else:\n",
    "            file1 = open(file_name, 'w')\n",
    "        \n",
    "        for i in news_list:\n",
    "            file1.write(i+\"\\n\")\n",
    "            \n",
    "        file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def make_f(newlist):\n",
    "    # 파일명\n",
    "    menu_list=['a3.txt','c3.txt','b3.txt',\n",
    "          'd3.txt','e3.txt','g3.txt']\n",
    "    for idx, i in enumerate(menu_list):\n",
    "        f = open(i,'w')\n",
    "        for j in newlist[idx]:\n",
    "            msg = j\n",
    "            f.write(j+\"\\n\")\n",
    "        f.close()\n",
    "        #main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스포츠 메뉴의 구조가 달라 따로 구함\n",
    "def sport(Sports_list, input_day):\n",
    "\n",
    "    for i in range(3):\n",
    "        params={\n",
    "            'page':i+1,\n",
    "            's_mcd':'0107'\n",
    "        }\n",
    "        resp = requests.get('https://www.ytn.co.kr/photo/photo_list.php',params=params)\n",
    "        soup = BeautifulSoup(resp.text, 'lxml')\n",
    "        #리스트\n",
    "        sec_tag = soup.find(\"div\", id=\"ytn_list_v2014\")\n",
    "        dl_tag=sec_tag.find_all(\"dl\",class_='photo_list')\n",
    "        \n",
    "        for j in dl_tag:\n",
    "            # 날짜\n",
    "            date_tag = j.find('dd',class_='date')\n",
    "            date_d = date_tag.text\n",
    "            date = date_d.split(\" \")\n",
    "            date = date[0].replace(\"[\",\"\")\n",
    "\n",
    "\n",
    "            if input_day == date:\n",
    "                dt_tag =j.find_all('dt')\n",
    "                a_tag = dt_tag[0].find(\"a\")\n",
    "                title = a_tag.text\n",
    "                \n",
    "                \n",
    "                if title in Sports_list:\n",
    "                    continue\n",
    "                    \n",
    "                Sports_list.append(title)\n",
    "                \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ytn(i_date):\n",
    "    \n",
    "    year = str(i_date)[:4]\n",
    "    month = str(i_date)[4:6]\n",
    "    day = str(i_date)[6:]\n",
    "    input_day = year + \"-\" + month + \"-\" + day\n",
    "    \n",
    "    # YTN사이트\n",
    "    url = 'https://www.ytn.co.kr/news/news_list_0101.html'\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, 'lxml')\n",
    "    menu = soup.find(\"ul\",id='sub_1')\n",
    "    li_tag = menu.find_all('li',recursive=False)\n",
    "\n",
    "    a=[]\n",
    "    politics_list=[]\n",
    "    economy_list=[]\n",
    "    social_list=[]\n",
    "    domestic_list=[]\n",
    "    International_list=[]\n",
    "    Science_list=[]\n",
    "    Sports_list=[]\n",
    "    culture_list =[]\n",
    "\n",
    "\n",
    "    # 카테고리마다 뉴스 제목 리스트\n",
    "    newlist = [politics_list,economy_list,social_list,domestic_list,\n",
    "               International_list,Science_list,culture_list]\n",
    "\n",
    "\n",
    "    #3,4,5,6,7,8,9,10,11\n",
    "    url2 = 'https://www.ytn.co.kr/news/news_list.php?'\n",
    "    for idx in range(3,11,1):\n",
    "        #메뉴이동\n",
    "        a_tag =li_tag[idx].find('a')\n",
    "        move = a_tag.get('href')\n",
    "        a = re.find_all('\\d+',move)\n",
    "\n",
    "        if idx == 10:\n",
    "            sport(Sports_list, input_day)\n",
    "            newlist.remove(domestic_list)\n",
    "            newlist.remove(Science_list)\n",
    "            newlist.append(Sports_list)\n",
    "            make_f(newlist)\n",
    "            break\n",
    "\n",
    "\n",
    "        for i in range(3):\n",
    "            params={\n",
    "                'page':i+1,\n",
    "                's_mcd':a[0]\n",
    "            }\n",
    "            resp = requests.get(url2,params=params)\n",
    "            soup = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "            #리스트\n",
    "            sec_tag = soup.find(\"div\", id=\"ytn_list_v2014\")\n",
    "\n",
    "            span_tag=sec_tag.find_all(\"dl\", class_=\"news_list_v2014\")\n",
    "\n",
    "            for j in span_tag:\n",
    "                # 날짜\n",
    "                date_tag = j.find('dd',class_='date')\n",
    "                date_d = date_tag.text\n",
    "                date = date_d.split(\" \")\n",
    "                date = date[0].replace(\"[\",\"\")\n",
    "\n",
    "\n",
    "                if input_day == date:\n",
    "                    dt_tag =j.find_all('dt')\n",
    "                    a_tag = dt_tag[0].find(\"a\")\n",
    "                    title = a_tag.text\n",
    "\n",
    "\n",
    "                    if title in newlist[idx-3]:\n",
    "                        continue\n",
    "                    if idx == 6 or idx == 8:\n",
    "                        newlist[6].append(title)\n",
    "\n",
    "                    else:    \n",
    "                        newlist[idx-3].append(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##########프로젝트 MBN 홈페이지 긁어오는 함수 코드##########\n",
    "############################################################\n",
    "\n",
    "def MBN_News_Function(i_date):\n",
    "    \n",
    "    input_date = str(i_date)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    reresult_list=[]\n",
    "    while count<7:\n",
    "        \n",
    "        if count==0:\n",
    "            category_s='politics'\n",
    "            MBN_url='https://www.mbn.co.kr/news/politics/'\n",
    "        \n",
    "        if count==1:\n",
    "            category_s='economy'\n",
    "            MBN_url='https://www.mbn.co.kr/news/economy/'\n",
    "\n",
    "        elif count==2:\n",
    "            category_s='society'\n",
    "            MBN_url='https://www.mbn.co.kr/news/society/'\n",
    "\n",
    "        elif count==3:\n",
    "            category_s='world'\n",
    "            MBN_url='https://www.mbn.co.kr/news/world/'\n",
    "\n",
    "        elif count==4:\n",
    "            category_s='culture'\n",
    "            MBN_url='https://www.mbn.co.kr/news/culture/'\n",
    "\n",
    "        elif count==5:\n",
    "            category_s='entertain'\n",
    "            MBN_url='https://www.mbn.co.kr/news/entertain/'        \n",
    "\n",
    "        elif count==6:\n",
    "            category_s='sports'\n",
    "            MBN_url='https://www.mbn.co.kr/news/sports/'\n",
    "\n",
    "        result_list=[]\n",
    "        for i in range(1,8): #페이지번호\n",
    "            params={\n",
    "                'page':i,\n",
    "                'vod':None,\n",
    "                'category':category_s\n",
    "            }\n",
    "            resp=requests.get(MBN_url, params=params)\n",
    "\n",
    "            resp.encoding='eur-kr'\n",
    "            \n",
    "            soup=BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "            MBN_news_list=soup.select('div.list_area > dl.article_list > dt.tit > a')\n",
    "            MBN_news_list_date=soup.select('div.list_area > dl.article_list > dd.desc > span.date')\n",
    "\n",
    "            news_list=[]\n",
    "            for news,date in zip(MBN_news_list, MBN_news_list_date):\n",
    "\n",
    "                news=news.text.strip() #제목 내 띄어쓰기 제거\n",
    "                news=news.replace(\"\\'\" , \" \") #제목 내 따옴표 제거\n",
    "                news=news.replace(\"♥\" , \" \") #제목 내 따옴표 제거\n",
    "                news=news.replace(\"[포토]\",\" \") #제목 내 [포토] 제거\n",
    "\n",
    "                news_day=date.text[0:10].replace(\"-\",\"\")\n",
    "\n",
    "                if news_day == input_date: #날짜 수정하면 됨\n",
    "                    news_list.append(news)\n",
    "            result_list.extend(news_list)\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "        reresult_list.append(result_list)\n",
    "    return reresult_list\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "#########프로젝트 MBN 메모장으로 꺼내오는 함수 코드#########\n",
    "############################################################\n",
    "\n",
    "\n",
    "def mbn(day):\n",
    "\n",
    "    title_list=['a','b','c','d','e','f','g']\n",
    "    #나중에 j를 e로 바꿔서 추가해야함!\n",
    "\n",
    "    f_list=[]\n",
    "    for title in title_list:\n",
    "        f=open('{}4.txt'.format(title),'w', -1, 'utf-8')    \n",
    "        f_list.append(f)\n",
    "\n",
    "        \n",
    "    MBN=MBN_News_Function(day)   # 2중리스트로 옴. [[1,2,3...] , [1,2,3...] , [1,2,3...]]\n",
    "\n",
    "\n",
    "    for i, f in zip(MBN,f_list):\n",
    "        for j in i:\n",
    "            f.write(j)\n",
    "            f.write('\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아시아경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asia(date):\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    while count<6:\n",
    "        day = int(datetime.today().strftime(\"%Y%m%d\")) - int(date)\n",
    "        \n",
    "        if count==0:\n",
    "            ASIA_url='https://www.asiae.co.kr/list/politics-all'\n",
    "            \n",
    "        if count==1:\n",
    "            ASIA_url='https://www.asiae.co.kr/list/economy-all'\n",
    "\n",
    "        elif count==2:\n",
    "            ASIA_url='https://www.asiae.co.kr/list/society-all'\n",
    "\n",
    "        elif count==3:\n",
    "            ASIA_url='https://www.asiae.co.kr/list/world-all'\n",
    "\n",
    "        elif count==4:\n",
    "            ASIA_url='https://www.asiae.co.kr/list/life-all'\n",
    "\n",
    "        elif count==5:\n",
    "            ASIA_url='https://www.asiae.co.kr/list/entertainment-sports-all'\n",
    "        \n",
    "        page_num = 1\n",
    "        check = 0\n",
    "        result_list = []\n",
    "        \n",
    "        while True: #페이지번호\n",
    "            Ex_url = ASIA_url + '/' + str(page_num)\n",
    "            resp=requests.get(Ex_url) # 1페이지부터 탐색\n",
    "            resp.encoding='eur-kr'\n",
    "            soup=BeautifulSoup(resp.text, 'lxml')\n",
    "            ASIA_news_list = soup.select('div.cont_listarea > div > div.listsm_type > h3.l_fsttit > a')\n",
    "            ASIA_news_time = soup.select('div.cont_listarea > div > div.listsm_type > span')\n",
    "            \n",
    "            \n",
    "            for li, time in zip(ASIA_news_list, ASIA_news_time):\n",
    "                \n",
    "                if(time.text == str(day + 1) +'일 전'):\n",
    "                    check += 1\n",
    "                    break\n",
    "                elif(time.text == str(day) + '일 전'):\n",
    "                    result_list.append(li.text.strip())\n",
    "\n",
    "            if(check == 1): \n",
    "                break\n",
    "                    \n",
    "            page_num += 1 # Page 넘기기\n",
    "            \n",
    "        \n",
    "        \n",
    "        title_list=['a','b','c','d','e','f']\n",
    "        \n",
    "        file_name = title_list[count] + '5.txt'\n",
    "        f = open(file_name, \"w+t\") # 무조건 처음 시작할 때 지우고 다시만든다.\n",
    "        for i in result_list:\n",
    "            f.write(i)\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-cc5f06dd16c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_sbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mjtbc_news_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mget_ytn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0masia\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-88e9635693d9>\u001b[0m in \u001b[0;36mget_ytn\u001b[1;34m(i_date)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmenu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sub_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mli_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmenu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "date = 20200924\n",
    "\n",
    "get_sbs(date)\n",
    "jtbc_news_list(date)\n",
    "get_ytn(date)\n",
    "mbn(date)\n",
    "asia(date)\n",
    "\n",
    "# # ################################# 텍스트에 Data 저장된 상태\n",
    "txt_save_list = ['정치.txt', '경제.txt', '사회.txt', '국제.txt', '생활&문화.txt', '연예.txt', '스포츠.txt']\n",
    "\n",
    "txt_file_list = [\n",
    "    'a1.txt', 'a2.txt', 'a3.txt', 'a4.txt', 'a5.txt',\n",
    "    'b1.txt', 'b2.txt', 'b3.txt', 'b4.txt', 'b5.txt',\n",
    "    'c1.txt', 'c2.txt', 'c3.txt', 'c4.txt', 'c5.txt',\n",
    "    'd1.txt', 'd2.txt', 'd3.txt', 'd4.txt', 'd5.txt',\n",
    "    'e1.txt', 'e2.txt', 'e3.txt', 'e4.txt', 'e5.txt',\n",
    "    'f1.txt', 'f2.txt', 'f3.txt', 'f4.txt', 'f5.txt',\n",
    "    'g1.txt', 'g2.txt', 'g3.txt', 'g4.txt', 'g5.txt' \n",
    "]\n",
    "\n",
    "title_name_list=['정치','경제','사회','국제','생활&문화','연예','스포츠']\n",
    "\n",
    "\n",
    "category_index_point = 0\n",
    "\n",
    "\n",
    "while category_index_point < 7: # 카테고리 탐색\n",
    "    \n",
    "    SBS = []    \n",
    "    JTBC= []    \n",
    "    YTN = []   \n",
    "    MBN= []     \n",
    "    ASIA = []   \n",
    "                                                                      #txt_file_list  index 들어가는 값들\n",
    "    f = open(txt_file_list[category_index_point * 5] , 'r')           # 1번 파일: 0, 5, 10, 15, 20, 25, 30\n",
    "    SBS = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    f = open(txt_file_list[(category_index_point*5) + 1] , 'r')       # 2번 파일: 1, 6, 11, 16, 21, 26, 31\n",
    "    JTBC = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    if((category_index_point * 5) + 2) == 27:                         # idex=27인 f3파일 없음.\n",
    "        pass\n",
    "    else:\n",
    "        f = open(txt_file_list[(category_index_point*5) + 2] , 'r')   # 3번 파일: 2, 7, 12, 17, 22,  , 32\n",
    "        YTN = f.read()\n",
    "        f.close()\n",
    "    \n",
    "    f = open(txt_file_list[(category_index_point*5) + 3] , 'r', encoding='utf-8')  # 4번 파일: 3, 8, 13, 18, 23, 28, 33\n",
    "    MBN = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    if((category_index_point * 5) + 4) == 34:\n",
    "        pass\n",
    "    else:\n",
    "        f = open(txt_file_list[(category_index_point*5) + 4] , 'r')   # 5번 파일: 4, 9, 14, 19, 24, 29, 34\n",
    "        ASIA = f.read()\n",
    "        f.close()\n",
    "    \n",
    "    \n",
    "    # 카테고리별로 뉴스사 데이터 합치기.\n",
    "    Category_sum = []\n",
    "    \n",
    "    if((category_index_point * 5) + 2) != 27 and ((category_index_point * 5) + 4) != 34:\n",
    "        Category_sum = SBS + JTBC + YTN + MBN + ASIA\n",
    "        \n",
    "    elif ((category_index_point * 5) + 2) == 27:\n",
    "        Category_sum = SBS + JTBC + MBN + ASIA\n",
    "    \n",
    "    elif ((category_index_point * 5) + 4) == 34:\n",
    "        Category_sum = SBS + JTBC + YTN + MBN\n",
    "    \n",
    "\n",
    "    f = open(txt_save_list[category_index_point], 'w+t')\n",
    "    f.write(Category_sum)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    # 뒤에 전체 키워드 뽑기위해서 따로 total.txt 생성\n",
    "    if os.path.exists('total.txt'): # total.txt 가 존재하면 내용추가(a)\n",
    "        f1 = open('total.txt', 'a')\n",
    "        f1.write(Category_sum)\n",
    "        f1.close()\n",
    "    else:\n",
    "        f1 = open('total.txt', 'w') # total.txt가 없으면 새로 생성해서 쓰기.\n",
    "        f1.write(Category_sum)\n",
    "        f1.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    category_index_point += 1 # 다음 카테고리\n",
    "\n",
    "    \n",
    "    #### 순위를 찾아내기위한 문장 리스트###\n",
    "category_index_point = 0\n",
    "\n",
    "while category_index_point < 7:\n",
    "\n",
    "    line_list=[]\n",
    "    \n",
    "    f = open(txt_save_list[category_index_point], 'r')\n",
    "    lines = f.read()\n",
    "    line_list = lines.split('\\n')\n",
    "    \n",
    "    \n",
    "    nlpy = Okt()\n",
    "    nouns = nlpy.nouns(lines)\n",
    "    count = Counter(nouns)\n",
    "\n",
    "    tag_count = []\n",
    "    \n",
    "    for n, c in count.most_common(100):\n",
    "        dics = {'키워드' : n, '중복수' : c}\n",
    "        if len(dics['키워드']) >= 2 and len(tag_count) <= 100:\n",
    "            tag_count.append(dics)\n",
    "            \n",
    "\n",
    "    #### 순위 3위까지의 리스트#####\n",
    "    rank=[]\n",
    "\n",
    "    \n",
    "    print('---------------카테고리[{}]--------------------'.format(title_name_list[category_index_point]) )\n",
    "    for idx,tag in enumerate(tag_count):    \n",
    "     #### 순위 3위까지의 리스트 뽑아내기 && 기사 뽑아내기#####\n",
    "        if(idx == 0 or idx == 1 or idx == 2 ):\n",
    "            rank.append(tag['키워드'])   \n",
    "    \n",
    "    print(pd.DataFrame(tag_count[:20], index = list(range(1,21)) ))\n",
    "\n",
    "            \n",
    "    print(\"\\n ********** 순위 ************* \\n\")  \n",
    "    print('\\t 1순위:{}'.format(rank[0]))\n",
    "    print('\\t 2순위:{}'.format(rank[1]))\n",
    "    print('\\t 3순위:{}'.format(rank[2]))  \n",
    "    \n",
    "    rank_li=[]\n",
    "    \n",
    "    temp=0\n",
    "    for line_li in line_list:\n",
    "        if rank[0] in line_li:\n",
    "            rank_li.append(line_li+'\\n')\n",
    "            break\n",
    "            \n",
    "\n",
    "    temp=0\n",
    "    for line_li in line_list:\n",
    "        if rank[1] in line_li:\n",
    "            rank_li.append(line_li+'\\n')\n",
    "            break\n",
    "            \n",
    "            \n",
    "    temp=0\n",
    "    for line_li in line_list:\n",
    "        if rank[2] in line_li:\n",
    "            rank_li.append(line_li+'\\n') \n",
    "            break\n",
    "             \n",
    "                \n",
    "    print(\"\\n ********** 기사 ************* \\n\")\n",
    "    print(rank_li[0])\n",
    "    print(rank_li[1])\n",
    "    print(rank_li[2])\n",
    "\n",
    "    \n",
    "    new_list = []\n",
    "    \n",
    "    for tag in tag_count:\n",
    "        c = (tag['키워드'], tag['중복수'])\n",
    "        new_list.append(c)\n",
    "        \n",
    "        \n",
    "    mask1 = np.array(Image.open(\"cloud.png\"))\n",
    "    font_path = 'C:/Windows/Fonts/malgun.ttf';\n",
    "    wc = WordCloud(font_path = font_path, background_color = 'white', \n",
    "        width=800, height=600, mask = mask1)\n",
    "    cloud = wc.generate_from_frequencies(dict(new_list))\n",
    "    plt.figure(figsize=(10,8), dpi=100)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cloud)\n",
    "    plt.show()\n",
    "    \n",
    "    category_index_point += 1\n",
    "    \n",
    "    \n",
    "    print('\\n' * 2)\n",
    "    \n",
    "\n",
    "f3 = open('total.txt', 'r')\n",
    "\n",
    "lines = f3.readline()\n",
    "news_topic_count = 1\n",
    "\n",
    "line_list.clear()\n",
    "while lines:\n",
    "    \n",
    "    lines = f3.readline()\n",
    "    news_topic_count += 1\n",
    "\n",
    "\n",
    "print('전체 기사 수 :{}'.format(news_topic_count))\n",
    "\n",
    "f3.close()\n",
    "    \n",
    "\n",
    "f = open('total.txt', 'r')\n",
    "lines = f.read()\n",
    "#### 순위를 찾아내기위한 문장 리스트###\n",
    "line_list = lines.split('\\n')\n",
    "f.close()\n",
    "\n",
    "nlpy = Okt()\n",
    "nouns = nlpy.nouns(lines)\n",
    "count = Counter(nouns)\n",
    "\n",
    "total_tag_count = []\n",
    "\n",
    "\n",
    "for n, c in count.most_common(100):\n",
    "    dics = {'키워드' : n, '중복수' : c}\n",
    "    if len(dics['키워드']) >= 2 and len(total_tag_count) <= 100:\n",
    "        total_tag_count.append(dics)\n",
    "        \n",
    "rank=[]\n",
    "\n",
    "\n",
    "print('---------------카테고리[전체 ]--------------------' )\n",
    "for idx,tag in enumerate(total_tag_count):\n",
    " #### 순위 3위까지의 리스트 뽑아내기 && 기사 뽑아내기#####\n",
    "    if(idx == 0 or idx == 1 or idx == 2 ):\n",
    "        rank.append(tag['키워드'])\n",
    "          \n",
    "print(pd.DataFrame(total_tag_count[:20], index = list(range(1,21)) ))\n",
    "\n",
    "print(\"\\n ********** 순위 ************* \\n\")  \n",
    "print('\\t 1순위:{}'.format(rank[0]))\n",
    "print('\\t 2순위:{}'.format(rank[1]))\n",
    "print('\\t 3순위:{}'.format(rank[2]))  \n",
    "\n",
    "rank_li.clear() \n",
    "\n",
    "\n",
    "temp=0\n",
    "for line_li in line_list:\n",
    "    if rank[0] in line_li:\n",
    "        rank_li.append(line_li+'\\n')\n",
    "        break\n",
    "\n",
    "temp=0\n",
    "for line_li in line_list:\n",
    "    if rank[1] in line_li:\n",
    "        rank_li.append(line_li+'\\n')\n",
    "        break\n",
    "        \n",
    "temp=0\n",
    "for line_li in line_list:\n",
    "    if rank[2] in line_li:\n",
    "        rank_li.append(line_li+'\\n')  \n",
    "        break\n",
    "              \n",
    "            \n",
    "print(\"\\n ********** 기사 ************* \\n\")\n",
    "print(rank_li[0])\n",
    "print(rank_li[1])\n",
    "print(rank_li[2])\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for tag in total_tag_count:\n",
    "    c = (tag['키워드'], tag['중복수'])\n",
    "    new_list.append(c)\n",
    "\n",
    "\n",
    "mask2 = np.array(Image.open(\"head.png\"))\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf';\n",
    "wc = WordCloud(font_path = font_path, background_color = 'white', \n",
    "    width=800, height=600, mask = mask2)\n",
    "cloud = wc.generate_from_frequencies(dict(new_list))\n",
    "plt.figure(figsize=(10,8), dpi = 100)\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
